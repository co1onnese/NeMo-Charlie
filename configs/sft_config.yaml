# Optimized SFT config for DeepSeek-V3.2-Exp (long-context, sparse attention)

base_model: "deepseek-ai/DeepSeek-V3.2-Exp"
dataset_path: "data/hf_datasets/sft_dataset"
tokenizer: null  # use base tokenizer
max_length: 65536              # supports extended context (adjust to your GPU memory)
context_window: 131072         # full model context capacity for DSA
rope_scaling: "dynamic"        # enable scalable positional encoding
trust_remote_code: true

# Training
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16
num_train_epochs: 2
learning_rate: 1.5e-4
weight_decay: 0.01
warmup_steps: 200
lr_scheduler_type: "cosine"
save_total_limit: 2

# Precision and quantization
load_in_4bit: true
bnb_quant_type: "nf4"
bnb_compute_dtype: "bfloat16"  # DeepSeek uses BF16 kernels
fp16: false
bf16: true

# LoRA / PEFT
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
bias: "none"

# Optimized for sparse attention kernels
gradient_checkpointing: true
gradient_checkpointing_policy: "block"
attention_impl: "flash"        # compatible with FlashMLA/DeepSeek kernels

# Logging and eval
logging_steps: 50
eval_steps: 500
save_steps: 500
output_dir: "checkpoints/sft-deepseek-v3.2exp-longctx"
use_wandb: true
wandb_project: "DeepSeek_SFT_Financial_Trading"