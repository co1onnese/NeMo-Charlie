================================================================================
                    MODEL STORAGE ARCHITECTURE
================================================================================

                        STORAGE FLOW DIAGRAM
                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HUGGINGFACE SERVERS                             â”‚
â”‚                    deepseek-ai/DeepSeek-V3.2-Exp                       â”‚
â”‚                          (~20-40 GB model)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ First Run Downloads
                             â”‚ (train_sft.py or tokenize_and_shard.py)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LOCAL HUGGINGFACE CACHE                            â”‚
â”‚            /root/.cache/huggingface/hub/                                â”‚
â”‚                                                                          â”‚
â”‚  models--deepseek-ai--DeepSeek-V3.2-Exp/                               â”‚
â”‚  â”œâ”€â”€ snapshots/                                                         â”‚
â”‚  â”‚   â””â”€â”€ [commit-hash]/                                                â”‚
â”‚  â”‚       â”œâ”€â”€ config.json                                               â”‚
â”‚  â”‚       â”œâ”€â”€ tokenizer.json                                            â”‚
â”‚  â”‚       â”œâ”€â”€ model-00001-of-00008.safetensors  (~2.5 GB)              â”‚
â”‚  â”‚       â”œâ”€â”€ model-00002-of-00008.safetensors  (~2.5 GB)              â”‚
â”‚  â”‚       â”œâ”€â”€ model-00003-of-00008.safetensors  (~2.5 GB)              â”‚
â”‚  â”‚       â”œâ”€â”€ ...                                                       â”‚
â”‚  â”‚       â””â”€â”€ model-00008-of-00008.safetensors  (~2.5 GB)              â”‚
â”‚  â””â”€â”€ refs/                                                              â”‚
â”‚                                                                          â”‚
â”‚  SIZE: ~20-40 GB                                                        â”‚
â”‚  STATUS: âŒ NOT DOWNLOADED YET                                          â”‚
â”‚  SHARED: Yes (used by all projects on this machine)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Loaded During Training
                             â”‚ (base model frozen)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRAINING PROCESS                                â”‚
â”‚                   (TRL SFTTrainer + PEFT/LoRA)                         â”‚
â”‚                                                                          â”‚
â”‚  Base Model (frozen) â”€â”€â”€â”€â”€â”                                            â”‚
â”‚                            â”‚                                             â”‚
â”‚                            â”œâ”€â”€â–¶ Add LoRA Adapters                       â”‚
â”‚                            â”‚                                             â”‚
â”‚  Training Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚                                                                          â”‚
â”‚  Output: LoRA Adapter Weights (~100-500 MB)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Saves Adapters Only
                             â”‚ (not full model)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINE-TUNED MODEL OUTPUT                              â”‚
â”‚           /opt/SFT-Charlie/checkpoints/                                 â”‚
â”‚                                                                          â”‚
â”‚  sft-deepseek-v3.2exp-longctx/                                         â”‚
â”‚  â”œâ”€â”€ adapter_config.json              (~1 KB)                          â”‚
â”‚  â”œâ”€â”€ adapter_model.safetensors        (~300 MB) â† âœ… FINE-TUNED        â”‚
â”‚  â”œâ”€â”€ config.json                      (~5 KB)                          â”‚
â”‚  â”œâ”€â”€ tokenizer.json                   (~2 MB)   â† Extended             â”‚
â”‚  â”œâ”€â”€ tokenizer_config.json            (~1 KB)                          â”‚
â”‚  â”œâ”€â”€ special_tokens_map.json          (~1 KB)                          â”‚
â”‚  â”œâ”€â”€ training_args.bin                (~5 KB)                          â”‚
â”‚  â”œâ”€â”€ manifest.json                    (~10 KB)  â† Reproducibility      â”‚
â”‚  â”œâ”€â”€ trainer_state.json               (~10 KB)                         â”‚
â”‚  â”‚                                                                       â”‚
â”‚  â””â”€â”€ checkpoint-500/                  (~300 MB)                         â”‚
â”‚      â”œâ”€â”€ adapter_model.safetensors                                     â”‚
â”‚      â””â”€â”€ ...                                                            â”‚
â”‚                                                                          â”‚
â”‚  SIZE: ~100-500 MB (LoRA adapters)                                     â”‚
â”‚  SIZE: ~1-2 GB (with checkpoints)                                      â”‚
â”‚  STATUS: â³ Created after training                                      â”‚
â”‚  PORTABLE: Yes (can copy to other machines)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                        INFERENCE / EVALUATION
                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TO USE THE FINE-TUNED MODEL, YOU NEED BOTH:                â”‚
â”‚                                                                          â”‚
â”‚  1. Base Model (20 GB)                                                  â”‚
â”‚     /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V3.2-Expâ”‚
â”‚                                                                          â”‚
â”‚  2. LoRA Adapters (500 MB)                                             â”‚
â”‚     /opt/SFT-Charlie/checkpoints/sft-deepseek-v3.2exp-longctx/         â”‚
â”‚                                                                          â”‚
â”‚  The adapters are loaded ON TOP of the base model.                     â”‚
â”‚  Together they form your fine-tuned model.                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                        DISK SPACE BREAKDOWN
                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CURRENT (CPU Server)                                â”‚
â”‚                                                                          â”‚
â”‚  Total Disk:     48 GB                                                  â”‚
â”‚  Used:           10 GB                                                  â”‚
â”‚  Available:      38 GB  âš ï¸ TOO SMALL FOR MODEL!                        â”‚
â”‚                                                                          â”‚
â”‚  /opt/SFT-Charlie/     ~50 MB    (code + docs)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  REQUIRED (GPU Server)                                  â”‚
â”‚                                                                          â”‚
â”‚  Base Model:                    20-40 GB                                â”‚
â”‚  Your Data:                     ~2-5 GB                                 â”‚
â”‚  Fine-tuned Checkpoints:        ~2 GB                                   â”‚
â”‚  Logs & Cache:                  ~1-2 GB                                 â”‚
â”‚  Safety Buffer:                 ~10 GB                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  TOTAL MINIMUM:                 35-60 GB                                â”‚
â”‚  RECOMMENDED:                   100 GB                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                        IMPORTANT NOTES
                        
âœ… LoRA Training Advantage:
   â€¢ Only saves adapter weights (~300 MB)
   â€¢ Much smaller than full fine-tuning (~20 GB)
   â€¢ Base model shared across all projects
   
âš ï¸  Storage Optimization:
   â€¢ Set HF_HOME to external disk if needed
   â€¢ Reduce save_total_limit in config
   â€¢ Delete tokenized dataset after training starts
   â€¢ Clean up old checkpoints
   
ğŸ”„ Portability:
   â€¢ Fine-tuned adapters are portable
   â€¢ Copy checkpoints/ folder to another machine
   â€¢ Re-download base model on new machine (or copy cache)
   â€¢ Both pieces needed for inference


                        DOWNLOAD TIMING
                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  xml_to_jsonl.py         No download (just parses XML)                 â”‚
â”‚       â†“                                                                  â”‚
â”‚  convert_dataset.py      No download (creates Arrow dataset)           â”‚
â”‚       â†“                                                                  â”‚
â”‚  tokenize_and_shard.py   â¬‡ï¸ Downloads tokenizer only (~10 MB)          â”‚
â”‚       â†“                                                                  â”‚
â”‚  train_sft.py            â¬‡ï¸ Downloads FULL MODEL (~20-40 GB)           â”‚
â”‚       â†“                                                                  â”‚
â”‚  evaluate_sft.py         Uses cached model OR fine-tuned adapters      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                        RECOMMENDATIONS
                        
ğŸ¯ CURRENT CPU SERVER (38 GB):
   âœ… DO: Process XML files, convert to JSONL
   âœ… DO: Create HuggingFace dataset (if dependencies installed)
   âŒ DON'T: Run tokenization (downloads model)
   âŒ DON'T: Run training
   
ğŸ¯ GPU SERVER:
   âœ… Check disk space: df -h / (need 50+ GB free)
   âœ… Set HF_HOME if limited space
   âœ… Run full pipeline
   âœ… Model auto-downloads on first use
   
ğŸ¯ AFTER TRAINING:
   âœ… Fine-tuned model in checkpoints/ (~500 MB)
   âœ… Copy to other machines if needed
   âœ… Base model cached (reused for future projects)

================================================================================
