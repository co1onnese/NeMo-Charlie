================================================================================
                    MODEL STORAGE ARCHITECTURE
================================================================================

                        STORAGE FLOW DIAGRAM
                        
┌─────────────────────────────────────────────────────────────────────────┐
│                         HUGGINGFACE SERVERS                             │
│                    deepseek-ai/DeepSeek-V3.2-Exp                       │
│                          (~20-40 GB model)                              │
└────────────────────────────┬────────────────────────────────────────────┘
                             │
                             │ First Run Downloads
                             │ (train_sft.py or tokenize_and_shard.py)
                             ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                      LOCAL HUGGINGFACE CACHE                            │
│            /root/.cache/huggingface/hub/                                │
│                                                                          │
│  models--deepseek-ai--DeepSeek-V3.2-Exp/                               │
│  ├── snapshots/                                                         │
│  │   └── [commit-hash]/                                                │
│  │       ├── config.json                                               │
│  │       ├── tokenizer.json                                            │
│  │       ├── model-00001-of-00008.safetensors  (~2.5 GB)              │
│  │       ├── model-00002-of-00008.safetensors  (~2.5 GB)              │
│  │       ├── model-00003-of-00008.safetensors  (~2.5 GB)              │
│  │       ├── ...                                                       │
│  │       └── model-00008-of-00008.safetensors  (~2.5 GB)              │
│  └── refs/                                                              │
│                                                                          │
│  SIZE: ~20-40 GB                                                        │
│  STATUS: ❌ NOT DOWNLOADED YET                                          │
│  SHARED: Yes (used by all projects on this machine)                    │
└────────────────────────────┬────────────────────────────────────────────┘
                             │
                             │ Loaded During Training
                             │ (base model frozen)
                             ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                         TRAINING PROCESS                                │
│                   (TRL SFTTrainer + PEFT/LoRA)                         │
│                                                                          │
│  Base Model (frozen) ─────┐                                            │
│                            │                                             │
│                            ├──▶ Add LoRA Adapters                       │
│                            │                                             │
│  Training Data ───────────┘                                             │
│                                                                          │
│  Output: LoRA Adapter Weights (~100-500 MB)                            │
└────────────────────────────┬────────────────────────────────────────────┘
                             │
                             │ Saves Adapters Only
                             │ (not full model)
                             ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                    FINE-TUNED MODEL OUTPUT                              │
│           /opt/SFT-Charlie/checkpoints/                                 │
│                                                                          │
│  sft-deepseek-v3.2exp-longctx/                                         │
│  ├── adapter_config.json              (~1 KB)                          │
│  ├── adapter_model.safetensors        (~300 MB) ← ✅ FINE-TUNED        │
│  ├── config.json                      (~5 KB)                          │
│  ├── tokenizer.json                   (~2 MB)   ← Extended             │
│  ├── tokenizer_config.json            (~1 KB)                          │
│  ├── special_tokens_map.json          (~1 KB)                          │
│  ├── training_args.bin                (~5 KB)                          │
│  ├── manifest.json                    (~10 KB)  ← Reproducibility      │
│  ├── trainer_state.json               (~10 KB)                         │
│  │                                                                       │
│  └── checkpoint-500/                  (~300 MB)                         │
│      ├── adapter_model.safetensors                                     │
│      └── ...                                                            │
│                                                                          │
│  SIZE: ~100-500 MB (LoRA adapters)                                     │
│  SIZE: ~1-2 GB (with checkpoints)                                      │
│  STATUS: ⏳ Created after training                                      │
│  PORTABLE: Yes (can copy to other machines)                            │
└─────────────────────────────────────────────────────────────────────────┘


                        INFERENCE / EVALUATION
                        
┌─────────────────────────────────────────────────────────────────────────┐
│              TO USE THE FINE-TUNED MODEL, YOU NEED BOTH:                │
│                                                                          │
│  1. Base Model (20 GB)                                                  │
│     /root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V3.2-Exp│
│                                                                          │
│  2. LoRA Adapters (500 MB)                                             │
│     /opt/SFT-Charlie/checkpoints/sft-deepseek-v3.2exp-longctx/         │
│                                                                          │
│  The adapters are loaded ON TOP of the base model.                     │
│  Together they form your fine-tuned model.                              │
└─────────────────────────────────────────────────────────────────────────┘


                        DISK SPACE BREAKDOWN
                        
┌─────────────────────────────────────────────────────────────────────────┐
│                     CURRENT (CPU Server)                                │
│                                                                          │
│  Total Disk:     48 GB                                                  │
│  Used:           10 GB                                                  │
│  Available:      38 GB  ⚠️ TOO SMALL FOR MODEL!                        │
│                                                                          │
│  /opt/SFT-Charlie/     ~50 MB    (code + docs)                         │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│                  REQUIRED (GPU Server)                                  │
│                                                                          │
│  Base Model:                    20-40 GB                                │
│  Your Data:                     ~2-5 GB                                 │
│  Fine-tuned Checkpoints:        ~2 GB                                   │
│  Logs & Cache:                  ~1-2 GB                                 │
│  Safety Buffer:                 ~10 GB                                  │
│  ─────────────────────────────────────                                 │
│  TOTAL MINIMUM:                 35-60 GB                                │
│  RECOMMENDED:                   100 GB                                  │
└─────────────────────────────────────────────────────────────────────────┘


                        IMPORTANT NOTES
                        
✅ LoRA Training Advantage:
   • Only saves adapter weights (~300 MB)
   • Much smaller than full fine-tuning (~20 GB)
   • Base model shared across all projects
   
⚠️  Storage Optimization:
   • Set HF_HOME to external disk if needed
   • Reduce save_total_limit in config
   • Delete tokenized dataset after training starts
   • Clean up old checkpoints
   
🔄 Portability:
   • Fine-tuned adapters are portable
   • Copy checkpoints/ folder to another machine
   • Re-download base model on new machine (or copy cache)
   • Both pieces needed for inference


                        DOWNLOAD TIMING
                        
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  xml_to_jsonl.py         No download (just parses XML)                 │
│       ↓                                                                  │
│  convert_dataset.py      No download (creates Arrow dataset)           │
│       ↓                                                                  │
│  tokenize_and_shard.py   ⬇️ Downloads tokenizer only (~10 MB)          │
│       ↓                                                                  │
│  train_sft.py            ⬇️ Downloads FULL MODEL (~20-40 GB)           │
│       ↓                                                                  │
│  evaluate_sft.py         Uses cached model OR fine-tuned adapters      │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘


                        RECOMMENDATIONS
                        
🎯 CURRENT CPU SERVER (38 GB):
   ✅ DO: Process XML files, convert to JSONL
   ✅ DO: Create HuggingFace dataset (if dependencies installed)
   ❌ DON'T: Run tokenization (downloads model)
   ❌ DON'T: Run training
   
🎯 GPU SERVER:
   ✅ Check disk space: df -h / (need 50+ GB free)
   ✅ Set HF_HOME if limited space
   ✅ Run full pipeline
   ✅ Model auto-downloads on first use
   
🎯 AFTER TRAINING:
   ✅ Fine-tuned model in checkpoints/ (~500 MB)
   ✅ Copy to other machines if needed
   ✅ Base model cached (reused for future projects)

================================================================================
