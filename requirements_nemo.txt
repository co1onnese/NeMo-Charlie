# NeMo and supporting packages for DeepSeek-V3 fine-tuning
#
# Installation order matters:
#   1. Install PyTorch first (via setup_env.sh with INSTALL_GPU_TORCH=true)
#   2. Then install these requirements
#
# Usage:
#   INSTALL_NEMO=true INSTALL_GPU_TORCH=true bash scripts/setup_env.sh

# Core NeMo package
# Note: Using base package instead of [all] to avoid optional dependencies
# that require compilation (like mamba-ssm) and may fail to build
nemo-toolkit

# PyTorch Lightning (required by NeMo but not auto-installed)
lightning>=2.1.0
pytorch-lightning>=2.1.0

# Megatron-Core (required for large model training)
megatron-core>=0.5.0

# Configuration management
omegaconf>=2.3.0
hydra-core>=1.3.0

# Tensor and model utilities
einops>=0.7.0
ninja>=1.11.0

# Triton (for custom kernels, needed for FP8 conversion)
triton>=2.1.0

# NVIDIA utilities (replaces deprecated pynvml)
nvidia-ml-py>=12.535.0

# NVIDIA Model Optimizer (required for NeMo LLM import)
nvidia-modelopt>=0.37.0

# Optional but recommended for performance
# apex  # Install manually from NVIDIA repo if needed
# flash-attn>=2.0.0  # Install manually if needed
# transformer-engine  # For optimized attention
# mamba-ssm  # State space models (requires C++ compilation, may fail on some systems)
